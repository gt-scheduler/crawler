name: Specified Crawling
on:
  workflow_dispatch:
    inputs:
      term:
        description: 'Enter terms to scrape, separated by commas'
        type: string
        required: true
concurrency:
  group: crawling

jobs:
  crawling:
    concurrency: ci-${{ github.ref }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v2
        with:
          persist-credentials: false

      - name: Checkout data
        uses: actions/checkout@v2
        with:
          persist-credentials: false
          ref: gh-pages
          path: ./data

      - name: Install
        run: yarn install --frozen-lockfile
        
      - name: Pip
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip' # caching pip dependencies

      - name: Pip Install
        run: pip install -r requirements.txt

      - name: Crawling
        run: yarn start
        env:
          LOG_FORMAT: json
          NUM_TERMS: 1
          SPECIFIED_TERM: ${{ inputs.term }}
          ALWAYS_SCRAPE_CURRENT_TERM: 0
          DETAILS_CONCURRENCY: 256
          DATA_FOLDER: ./data
          NODE_EXTRA_CA_CERTS: ${{ github.workspace }}/intermediate.pem
          
      - name: Revision
        run: python ./src/Revise.py

      - name: Upload
        uses: JamesIves/github-pages-deploy-action@releases/v4
        with:
          token: ${{ secrets.CRAWLER_DEPLOY_PERSONAL_ACCESS_TOKEN }}
          branch: gh-pages
          folder: ./data
          clean: true
          single-commit: true
          git-config-name: gt-scheduler-bot
          git-config-email: 89671168+gt-scheduler-bot@users.noreply.github.com
